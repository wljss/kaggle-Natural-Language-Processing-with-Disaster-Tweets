{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d92efa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#读入数据\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "trainData = pd.read_csv(\"./nlp-getting-started-data/train.csv\")\n",
    "trainData['text'] = trainData['text'].fillna('')\n",
    "\n",
    "trainKeyword=trainData.values[:,1]\n",
    "trainLocation=trainData.values[:,2]\n",
    "trainText=trainData.values[:,3]\n",
    "trainTarget=trainData.values[:,4]\n",
    "\n",
    "testData = pd.read_csv(\"./nlp-getting-started-data/test.csv\")\n",
    "testData['text'] = testData['text'].fillna('')\n",
    "\n",
    "testKeyword=testData.values[:,1]\n",
    "testLocation=testData.values[:,2]\n",
    "testText=testData.values[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c3133cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport seaborn as sns #一个Matplotlib高级数据可视化库，画统计图表\\n\\ntrainData['targetMean']=trainData.groupby('keyword')['target'].transform('mean')\\nfig = plt.figure(figsize=(8,27))\\nsns.countplot(y=trainData.sort_values(by='targetMean',ascending=False)['keyword'], hue=trainData.sort_values(by='targetMean',ascending=False)['target']) #按照targetMean排序，默认升序，ascending=False改为降序排序\\n\\nplt.legend(loc=1)\\n\\nplt.show()\\n\\ntrainData.drop(columns=['targetMean'], inplace=True) #inplace=True的时候在原来的数据上发生改变，默认为inplace=False不改变原数据\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import seaborn as sns #一个Matplotlib高级数据可视化库，画统计图表\n",
    "\n",
    "trainData['targetMean']=trainData.groupby('keyword')['target'].transform('mean')\n",
    "fig = plt.figure(figsize=(8,27))\n",
    "sns.countplot(y=trainData.sort_values(by='targetMean',ascending=False)['keyword'], hue=trainData.sort_values(by='targetMean',ascending=False)['target']) #按照targetMean排序，默认升序，ascending=False改为降序排序\n",
    "\n",
    "plt.legend(loc=1)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "trainData.drop(columns=['targetMean'], inplace=True) #inplace=True的时候在原来的数据上发生改变，默认为inplace=False不改变原数据\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23d1b97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizer分词器\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    cache_dir='./myModels', #下载的模型储存的位置\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e637d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, max_len=256): #初始化。\n",
    "        self.texts=texts\n",
    "        self.labels=labels\n",
    "        self.max_len=max_len\n",
    "    def __len__(self): #获取包含的样本数\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx): #返回索引idx处的数据集样本\n",
    "        text = str(self.texts[idx])\n",
    "        encoding = tokenizer.encode_plus( #用于文本编码，会返回：1.input_ids:token ID序列(包括[CLS](通常表示句子或文档开头，对应输入文本第一个词向量)和[SEP](对应最后一个词向量，分割不同句子，句子间用来表示分界点)) 2.token_type_ids句子区分标记 3.attention_mask指示有效token(为1)和无效token(为0)\n",
    "            text, #文本\n",
    "            add_special_tokens=True, #是否添加[CLS]和[SEP]\n",
    "            max_length=self.max_len, #最大序列长度\n",
    "            padding='max_length', #是否填充\n",
    "            truncation=True, #是否截断\n",
    "            return_attention_mask=True, #是否返回attention_mask\n",
    "            return_tensors='pt', #返回tf(Tensorflow的Tensor)、pt(PyTorch的)或np(Numpy的ndarray)\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].flatten() #.flatten()用于将多维数据降为一位数据\n",
    "        attention_mask = encoding['attention_mask'].flatten()\n",
    "        \n",
    "        if len(input_ids) > self.max_len:\n",
    "            input_ids = input_ids[:self.max_len]\n",
    "            attention_mask = attention_mask[:self.max_len]\n",
    "        elif len(input_ids) < self.max_len:\n",
    "            # 手动填充\n",
    "            pad_length = self.max_len - len(input_ids)\n",
    "            input_ids = torch.cat([\n",
    "                input_ids, \n",
    "                torch.full((pad_length,), tokenizer.pad_token_id, dtype=torch.long)\n",
    "            ])\n",
    "            attention_mask = torch.cat([\n",
    "                attention_mask, \n",
    "                torch.zeros(pad_length, dtype=torch.long)\n",
    "            ])\n",
    "        \n",
    "        # 添加长度检查\n",
    "        assert len(input_ids) == self.max_len, f\"input_ids length {len(input_ids)} != {self.max_len}\"\n",
    "        assert len(attention_mask) == self.max_len, f\"attention_mask length {len(attention_mask)} != {self.max_len}\"\n",
    "        \n",
    "        if self.labels is not None: #如果有labels则是训练集\n",
    "            labels = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "            return {\n",
    "                'input_ids':input_ids,\n",
    "                'attention_mask':attention_mask,\n",
    "                'labels':labels\n",
    "            }\n",
    "        else: #没有labels则是测试集\n",
    "            return {\n",
    "                'input_ids':input_ids,\n",
    "                'attention_mask':attention_mask\n",
    "            }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb8215ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#构造数据集，训练集和验证集\n",
    "from sklearn.model_selection import train_test_split\n",
    "trainDataList, testDataList, trainClassList, testClassList = train_test_split(trainData['text'], trainData['target'], test_size=0.98, random_state=233) #划分训练集和验证集，由于训练的有点慢所以test_size较大先试一下\n",
    "\n",
    "trainDataset = TweetDataset(trainDataList.tolist(), trainClassList.tolist()) #.tolist()可以将任意形状的Tensor转换为Python原生标量\n",
    "#当下标为8的数据划分到testClassList的话，这里运行print(trainClassList[8])会报错，这是以你为trainClassList 是一个 pandas Series，其索引可能不是连续的，不存在索引为8的数据\n",
    "testDataset = TweetDataset(testDataList.tolist(), testClassList.tolist())\n",
    "BATCH_SIZE=16\n",
    "trainLoader = DataLoader(trainDataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "testLoader = DataLoader(testDataset, batch_size = BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aefa155",
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载BERT模型\n",
    "from transformers import BertForSequenceClassification\n",
    "model = BertForSequenceClassification.from_pretrained( #BERT模型的序列分类版本\n",
    "    'bert-base-uncased', #BERT基础版本\n",
    "    cache_dir='./myModels', #下载的模型储存的位置,不设置默认会在用户文件夹下/.cache/torch/transformers里\n",
    "    num_labels = 2, #二分类\n",
    "    output_attentions = False, #不返回注意力权重矩阵\n",
    "    output_hidden_states = False #不返回所有隐藏层的输出，进返回最后一层\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('当前device为' + str(device))\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ec4f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch is 0i is 0\n",
      "epoch is 0i is 1\n",
      "epoch is 0i is 2\n",
      "epoch is 0i is 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31196\\1599326089.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#清空模型参数梯度\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#计算损失函数对模型参数的梯度\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#更新模型参数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\24508\\Anaconda3\\envs\\jqxx\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             )\n\u001b[0;32m    488\u001b[0m         torch.autograd.backward(\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         )\n\u001b[0;32m    491\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\24508\\Anaconda3\\envs\\jqxx\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m def grad(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#循环训练\n",
    "from transformers import AdamW\n",
    "epochs = 3 #迭代次数\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train() #训练模式\n",
    "    i=0\n",
    "    for batch in trainLoader:\n",
    "        print('epoch is ' + str(epoch) + ' i is ' + str(i))\n",
    "        i=i+1\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "    \n",
    "        optimizer.zero_grad() #清空模型参数梯度\n",
    "        loss = outputs[0]\n",
    "        loss.backward() #计算损失函数对模型参数的梯度\n",
    "        optimizer.step() #更新模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71d3b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()#预测模式"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jqxx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
